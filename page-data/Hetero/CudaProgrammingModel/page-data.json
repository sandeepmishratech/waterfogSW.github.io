{"componentChunkName":"component---src-templates-blog-post-js","path":"/Hetero/CudaProgrammingModel/","result":{"data":{"site":{"siteMetadata":{"title":"For The Record","author":"San Kim","siteUrl":"https://waterfogsw.github.io","comment":{"disqusShortName":"","utterances":"waterfogSW/waterfogSW.github.io"}}},"markdownRemark":{"id":"a9ce8003-9c58-5efe-b4a5-7803712f397e","excerpt":"2.1 Kernels CUDA에서는 C++의 함수를 지원하는데 이를 kernel이라 합니다. kernel 은 __ global __ 이라는 specifier를 통해 정의하며 선언시 다음과 같은 형식을 따릅니다. 상기 코드의 main함수에서 VecAdd()라는 이름의 kernel을 사용하는것을 확인 하실수 있는데 C++의 일반적인 함수들과 달리 <<<…, …>>> 와 같은 꺽쇠 세번안에 두개의 인자가 들어갑니다.  꺽쇠안의 첫번째 인자에는 CUDA block의 개수를 두번째 인자에는 block…","html":"<h2 id=\"21-kernels\" style=\"position:relative;\"><a href=\"#21-kernels\" aria-label=\"21 kernels permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1 Kernels</h2>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">// Kernel definition \n__global__ void VecAdd(float* A, float* B, float* C) {\n    int i = threadIdx.x; \n    C[i] = A[i] + B[i]; \n} \n\nint main() {\n    ... \n    // Kernel invocation with N threads \n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ... \n}</code></pre></div>\n<p>CUDA에서는 C++의 함수를 지원하는데 이를 kernel이라 합니다.</p>\n<p>kernel 은 __ global __ 이라는 specifier를 통해 정의하며 선언시 다음과 같은 형식을 따릅니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">__global__ type fucntionName(arg1,arg2,...)</code></pre></div>\n<p>상기 코드의 main함수에서 VecAdd()라는 이름의 kernel을 사용하는것을 확인 하실수 있는데 C++의 일반적인 함수들과 달리 &#x3C;&#x3C;&#x3C;…, …>>> 와 같은 꺽쇠 세번안에 두개의 인자가 들어갑니다. </p>\n<p>꺽쇠안의 첫번째 인자에는 CUDA block의 개수를 두번째 인자에는 block당 CUDA thread의 개수가 들어갑니다.</p>\n<h2 id=\"22-thread-hierarchy\" style=\"position:relative;\"><a href=\"#22-thread-hierarchy\" aria-label=\"22 thread hierarchy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2 Thread Hierarchy</h2>\n<p>CPU의 thread와 마찬가지로 CUDA의 thread는 작업기본 단위입니다. CUDA에서의 thread는 Block단위로 묶이고, 다시 이 Block은 Grid단위로 묶입니다.\n<img src=\"https://nyu-cds.github.io/python-gpu/fig/02-threadmapping.png\"></p>\n<p> 위의 그림과 같이 CUDA의 grid과 block은 1차원, 2차원 또는 3차원 세가지 구조로 정의 될수 있습니다. </p>\n<p>다음은 행렬A,B의 합을 행렬C에 저장하는 예제 코드입니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">// Kernel definition \n__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) { \n    int i = threadIdx.x;\n    int j = threadIdx.y;\n    C[i][j] = A[i][j] + B[i][j]; \n} \nint main() { \n    ... \n    // Kernel invocation with one block of N * N * 1 threads \n    int numBlocks = 1; \n    dim3 threadsPerBlock(N, N);\n    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;(A, B, C);\n    …\n}</code></pre></div>\n<p>CUDA에서는 사전정의된 변수를 통해 Block과 Thread에 접근할 수 있습니다.</p>\n<p>threadIdx.~ : block안의 thread에서 해당 thread의 인덱스를 가리킵니다.</p>\n<p>blockIdX.~  : gird안의 block에서 해당 block의 인덱스를 가리킵니다.</p>\n<p>blockDim.~  : block안의 thread의 개수를 가리킵니다.</p>\n<p>이에 따라 2차원의 block을 사용할때 blockIdx.x * blockDim.x + threadIdx.x를 통해 특정 thread를 가리킬수 있습니다. </p>\n<p>다음은 2차원의 block을 사용하여 앞선 코드와 동일한 동작을 하는 예제 코드입니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">// Kernel definition\n__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]){\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i &lt; N &amp;&amp; j &lt; N)\n        C[i][j] = A[i][j] + B[i][j];\n}\n\nint main(){\n    ...\n    // Kernel invocation\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);\n    ...\n}</code></pre></div>\n<p>MatAdd kernel에서 변수 i, j를 통해 thread에 접근하는 인덱스를 정의하였습니다. </p>\n<p>main함수에서는 1차원의 block을 사용할때와 달리 numBlocks의 자료형을 dim3로 정의된것을 확인하실수 있습니다. dim3은 unsigned int 형의 구조체로 x,y,z 3개의 구조체 변수를 가집니다.</p>\n<table>\n  <tr>\n   <td><strong>Dimension</strong>\n   </td>\n   <td><strong>Block of size</strong>\n   </td>\n   <td><strong>Thread ID of a thread of index</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>1\n   </td>\n   <td>Dx(thread per block)\n   </td>\n   <td>x\n   </td>\n  </tr>\n  <tr>\n   <td>2\n   </td>\n   <td>Dx , Dy\n   </td>\n   <td>x + y Dx\n   </td>\n  </tr>\n  <tr>\n   <td>3\n   </td>\n   <td>Dx , Dy , Dz\n   </td>\n   <td>x + y Dx + z Dx Dy \n   </td>\n  </tr>\n</table>\n<h2 id=\"23-memory-hierarchy\" style=\"position:relative;\"><a href=\"#23-memory-hierarchy\" aria-label=\"23 memory hierarchy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.3 Memory Hierarchy</h2>\n<p><img src=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-hierarchy.png\" alt=\"img\"></p>\n<p>CUDA에서 각 thread는 local메모리를 가지고 각 Thread Block은 block과 똑같은 life time을 갖고, block내의 모든 thread가 접근할 수 있는 shared memory를 가집니다. 또 모든 thread는 어느grid, block에 속하는지 관계없이 Global memory에 접근할 수 있습니다.</p>","frontmatter":{"title":"[CUDA]Programming Model","date":"February 21, 2021"}}},"pageContext":{"slug":"/Hetero/CudaProgrammingModel/","previous":null,"next":{"fields":{"slug":"/BackEnd/Mysql-in-WSL/"},"frontmatter":{"title":"[MySql] WSL에서 mysql사용환경 만들기"}}}}}